{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from math import ceil\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "#from torch_geometric.data import DenseDataLoader #To make use of this data loader, all graph attributes in the dataset need to have the same shape. In particular, this data loader should only be used when working with dense adjacency matrices.\n",
    "from torch_geometric.nn import DenseGCNConv as GCNConv, dense_diff_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'c:/Users/david/MT_data/extracted_patches/mutant_graphs_diffpool/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from c_PatchDataset_diffpool import PatchDataset\n",
    "dataset = PatchDataset(data_dir = data_dir)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: PatchDataset(100):\n",
      "====================\n",
      "Number of graphs: 100\n",
      "\n",
      "Data(x=[1300, 16], y=0, pos=[1300, 3], adj=[1300, 1300], w=[1300, 1300], mutant='AAAP')\n",
      "=============================================================\n",
      "Number of nodes: 1300\n",
      "Number of node features: 16\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of node features: {data.num_node_features}')\n",
    "#print(f'Number of edges: {data.num_edges}')\n",
    "#print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "#print(f'Contains self-loops: {data.has_self_loops()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
       "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 67\n",
      "Number of validation graphs: 17\n",
      "Number of test graphs: 16\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DenseDataLoader\n",
    "\n",
    "#max_nodes = 150\n",
    "#class MyFilter(object):\n",
    "#    def __call__(self, data):\n",
    "#        return data.num_nodes <= max_nodes\n",
    "#dataset = TUDataset('data', name='PROTEINS', transform=T.ToDense(max_nodes),\n",
    "#                    pre_filter=MyFilter())\n",
    "#dataset = dataset.shuffle()\n",
    "#n = (len(dataset) + 9) // 10\n",
    "batch_size = 2 # cannot change this at the moment\n",
    "\n",
    "n_train = ceil((4/6) * len(dataset))\n",
    "n_val = ceil((len(dataset) - n_train)/2)\n",
    "n_test = len(dataset) - n_train - n_val\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [n_train, n_val, n_test])\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of validation graphs: {len(val_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "#test_dataset = dataset[:n]\n",
    "#val_dataset = dataset[n:2 * n]\n",
    "#train_dataset = dataset[2 * n:]\n",
    "\n",
    "train_loader = DenseDataLoader(dataset = train_dataset, batch_size= batch_size)\n",
    "val_loader = DenseDataLoader(dataset = val_dataset, batch_size= batch_size)\n",
    "test_loader = DenseDataLoader(dataset = test_dataset, batch_size= batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[1300, 16], y=1, pos=[1300, 3], adj=[1300, 1300], w=[1300, 1300], mutant='AYCA')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[1300, 16], y=1, pos=[1300, 3], adj=[1300, 1300], w=[1300, 1300], mutant='VSAA')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels,\n",
    "                 normalize=False, lin=True):\n",
    "        super(GNN, self).__init__()\n",
    "        \n",
    "        self.convs = torch.nn.ModuleList()\n",
    "\n",
    "        # Each instance of this GNN will have 3 convolutional layers and three batch norm layers        \n",
    "        self.convs.append(GCNConv(in_channels, hidden_channels, normalize))\n",
    "        self.convs.append(GCNConv(hidden_channels, hidden_channels, normalize))\n",
    "        self.convs.append(GCNConv(hidden_channels, out_channels, normalize))\n",
    "\n",
    "\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        \n",
    "        #Feed the feature matrix and the adjacency matrix through the 3 conv and 3 bns layers\n",
    "        for step in range(len(self.convs)):\n",
    "            #x = self.bns[step](F.relu(self.convs[step](x, adj, mask)))\n",
    "            x = F.relu(self.convs[step](x, adj, mask))\n",
    "        return x\n",
    "\n",
    "\n",
    "class DiffPool(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiffPool, self).__init__()\n",
    "\n",
    "        #PoolGNN\n",
    "        num_nodes = [1095, 200, 100, 25, 1]\n",
    "        self.gnn1_pool = GNN(dataset.num_features, 64, num_nodes[1]) # --> S1 num_nodes[0] x num_nodes[1] (1095x200)\n",
    "        self.gnn2_pool = GNN(32, 64, num_nodes[2])                   # --> S2 num_nodes[1] x num_nodes[2] (200x100)\n",
    "        self.gnn3_pool = GNN(64, 32, num_nodes[3])                   # --> S3 num_nodes[2] x num_nodes[3] (100x25)\n",
    "        self.gnn4_pool = GNN(128, 64, num_nodes[4])                   # --> S4 num_nodes[3] x num_nodes[4] (25x1)\n",
    "\n",
    "        #EmbedGNN\n",
    "        self.num_features = [dataset.num_features, 32, 64, 128, 256]\n",
    "        self.gnn1_embed = GNN(self.num_features[0], self.num_features[0], self.num_features[1])             # --> X1 num_nodes[0] x num_features[1] (1000x32)   --matmul--> (200x32)\n",
    "        self.gnn2_embed = GNN(self.num_features[1], self.num_features[1], self.num_features[2], lin=False)  # --> X1 num_nodes[1] x num_features[2] (200x64)    --matmul--> (100x64)\n",
    "        self.gnn3_embed = GNN(self.num_features[2], self.num_features[2], self.num_features[3], lin=False)  # --> X1 num_nodes[2] x num_features[3] (100x128)   --matmul--> (25x128)\n",
    "        self.gnn4_embed = GNN(self.num_features[3], self.num_features[3], self.num_features[4], lin=False)  # --> X1 num_nodes[3] x num_features[4] (25x256)    --matmul--> (1x256)\n",
    "        self.gnn5_embed = GNN(self.num_features[4], self.num_features[4], self.num_features[4], lin=False)  # \n",
    "\n",
    "        # Final Classifier\n",
    "        self.lin1 = torch.nn.Linear(256, 64) \n",
    "        self.lin2 = torch.nn.Linear(64, 2)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        #print()\n",
    "        #print('=======================')\n",
    "        #print(f'input X: {x.shape}')\n",
    "        #print(f'input adj: {adj.shape}')\n",
    "\n",
    "        #Hierarchical Step #1\n",
    "        #print()\n",
    "        #print('First hierarchical step:')\n",
    "        s = self.gnn1_pool(x, adj, mask) # cluster assignment matrix\n",
    "        #print(f'S1 = {s.shape}')\n",
    "        x = self.gnn1_embed(x, adj, mask) # node feature embedding\n",
    "        #print(f'X1 = {x.shape}')\n",
    "        x, adj, l1, e1 = dense_diff_pool(x, adj, s, mask) # does the necessary matrix multiplications\n",
    "        #print(f'X Output 1 = {x.shape}')\n",
    "        #print(f'Adj Output 1 = {adj.shape}')\n",
    "\n",
    "        #Hierarchical Step #2\n",
    "        #print()\n",
    "        #print('Second hierarchical step:')\n",
    "        s = self.gnn2_pool(x, adj, mask) # cluster assignment matrix\n",
    "        #print(f'S2 = {s.shape}')\n",
    "        x = self.gnn2_embed(x, adj, mask) # node feature embedding\n",
    "        #print(f'X2 = {x.shape}')\n",
    "        x, adj, l1, e1 = dense_diff_pool(x, adj, s, mask) # does the necessary matrix multiplications\n",
    "        #print(f'X Output 2 = {x.shape}')\n",
    "        #print(f'Adj Output 2 = {adj.shape}')\n",
    "        \n",
    "        # Hierarchical Step #3\n",
    "        #print()\n",
    "        #print('Third hierarchical step:')\n",
    "        s = self.gnn3_pool(x, adj)\n",
    "        #print(f'S3: {s.shape}')\n",
    "        x = self.gnn3_embed(x, adj)\n",
    "        #print(f'X3: {x.shape}')\n",
    "        x, adj, l2, e2 = dense_diff_pool(x, adj, s)\n",
    "        #print(f'X Output 3 = {x.shape}')\n",
    "        #print(f'Adj Output 3 = {adj.shape}')\n",
    "\n",
    "        # Hierarchical Step #4\n",
    "        #print()\n",
    "        #print('Fourth hierarchical step:')\n",
    "        s = self.gnn4_pool(x, adj)\n",
    "        #print(f'S4: {s.shape}')\n",
    "        x = self.gnn4_embed(x, adj)\n",
    "        #print(f'X4: {x.shape}')\n",
    "        x, adj, l2, e2 = dense_diff_pool(x, adj, s)\n",
    "        #print(f'X Output 4 = {x.shape}')\n",
    "        #print(f'Adj Output 4 = {adj.shape}')\n",
    "\n",
    "        # Final Classification\n",
    "        #print()\n",
    "        #print('Final Classification')\n",
    "        x = x.mean(dim=1) # Pool the features of all nodes (global mean pool)  dim = 1 refers to columns\n",
    "        #print(f'X Output after mean= {x.shape}')\n",
    "\n",
    "        x = F.relu(self.lin1(x)) # Fully connected layer + relu\n",
    "        #print(f'X Output after lin1= {x.shape}')\n",
    "\n",
    "        x = self.lin2(x) # Reduction to num_classes\n",
    "        #print(f'X Output after lin2= {x.shape}')\n",
    "\n",
    "        not_log = F.softmax(x, dim=-1)\n",
    "        log = F.log_softmax(x, dim=-1)\n",
    "        #print(f'Softmax = {not_log}')\n",
    "        #print(f'LogSoftmax = {log}')\n",
    "\n",
    "        return log, l1 + l2, e1 + e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([2, 1300, 16])\n",
      "torch.Size([2, 1300, 1300])\n",
      "torch.Size([1, 1300, 16])\n",
      "torch.Size([1, 1300, 1300])\n",
      "Epoch: 001, Train Loss: 0.8673, Val Acc: 0.5882, Test Acc: 0.1875\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DiffPool().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "node_lengths = []\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        node_lengths.append(data.x.shape[0])\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        print(data.x.shape)\n",
    "        print(data.adj.shape)\n",
    "        output, _, _ = model(data.x, data.adj) #data.mask\n",
    "        loss = F.nll_loss(output, data.y.view(-1))\n",
    "        loss.backward()\n",
    "        loss_all += data.y.size(0) * loss.item()\n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data.x, data.adj)[0].max(dim=1)[1] #, data.mask\n",
    "        correct += pred.eq(data.y.view(-1)).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "best_val_acc = test_acc = 0\n",
    "for epoch in range(1, 2):\n",
    "    train_loss = train(epoch)\n",
    "    val_acc = test(val_loader)\n",
    "    if val_acc > best_val_acc:\n",
    "        test_acc = test(test_loader)\n",
    "        best_val_acc = val_acc\n",
    "    print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, '\n",
    "          f'Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#len(node_lengths)\n",
    "#plt.hist(node_lengths, bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min(node_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max(node_lengths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pyg_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5295f743bc4e47f7cb4c7d5e484c5cf6bb52e824ff35c3fecf2642e2b62ae0ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
