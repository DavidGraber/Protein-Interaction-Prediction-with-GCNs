{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages.\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'c:/Users/david/MT_data/extracted_patches/mutant_graphs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show an example Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(data_dir, 'AAAH_GraphPatch.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 6565)\n",
      "(6565, 1)\n",
      "0.0\n",
      "(1073, 1073)\n",
      "AAAH\n",
      "\n",
      "            Number of Nodes: 1073\n",
      "            Features: (1073, 16)\n",
      "            Adjacency Matrix: (1073, 1073)\n",
      "            Edge Weights (Geodesic Distances): (6565, 1)\n",
      "            Edge Index: (2, 6565)\n",
      "            Fitness: 0.0\n",
      "            Coordinates of Points: (1073, 3)\n",
      "            Mutant Name: AAAH\n"
     ]
    }
   ],
   "source": [
    "from f_helper_functions import *\n",
    "\n",
    "patch = load_object(path)\n",
    "print(patch.edge_index.shape)\n",
    "print(patch.edge_weight.shape)\n",
    "print(patch.fitness)\n",
    "print(patch.A.shape)\n",
    "print(patch.mutant)\n",
    "print()\n",
    "print(patch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define customized Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from torch.utils.data import Dataset\n",
    "from c_PatchDataset_regr import PatchDataset\n",
    "\n",
    "batch_size = 8\n",
    "dataset = PatchDataset(data_dir = data_dir)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: PatchDataset(1500):\n",
      "====================\n",
      "Number of graphs: 1500\n",
      "\n",
      "Data(x=[1049, 16], edge_index=[2, 6435], edge_attr=[6435, 1], y=1.611610009, pos=[1049, 3])\n",
      "=============================================================\n",
      "Number of nodes: 1049\n",
      "Number of node features: 16\n",
      "Number of edges: 6435\n",
      "Average node degree: 6.13\n",
      "Contains self-loops: True\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of node features: {data.num_node_features}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Contains self-loops: {data.has_self_loops()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6116, dtype=torch.float64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2276],\n",
       "        [0.1386],\n",
       "        [0.1039],\n",
       "        ...,\n",
       "        [0.0660],\n",
       "        [0.0647],\n",
       "        [0.0574]], dtype=torch.float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    0,    0,  ..., 1048, 1048, 1048],\n",
       "        [   0,    4,    2,  ..., 1037, 1046, 1024]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3169,  0.0202, -0.0707,  ..., -0.4254, -0.0092, -0.0602],\n",
       "        [-0.2913,  0.0608, -0.1049,  ..., -0.3702,  0.0567, -0.1223],\n",
       "        [-0.2959,  0.0752, -0.1396,  ..., -0.3760,  0.0288, -0.0965],\n",
       "        ...,\n",
       "        [-0.1887,  0.0367, -0.0865,  ..., -0.3686, -0.0074, -0.3037],\n",
       "        [-0.1788,  0.0696, -0.1506,  ..., -0.3097,  0.0227, -0.2670],\n",
       "        [-0.1877,  0.0759, -0.1713,  ..., -0.2963,  0.0164, -0.2393]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.3507,  3.2255,  9.1402],\n",
       "        [-5.6844,  2.0925,  9.0088],\n",
       "        [-5.7997,  2.8937,  9.1833],\n",
       "        ...,\n",
       "        [11.6539,  3.1238,  2.1397],\n",
       "        [11.5864,  3.7266,  2.1783],\n",
       "        [11.5499,  4.1333,  2.0281]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 1200\n",
      "Number of test graphs: 300\n"
     ]
    }
   ],
   "source": [
    "n_train = int(len(dataset)*0.8)\n",
    "n_test = len(dataset) - n_train \n",
    "\n",
    "trainset, testset = torch.utils.data.random_split(dataset, [n_train, n_test])\n",
    "\n",
    "print(f'Number of training graphs: {len(trainset)}')\n",
    "print(f'Number of test graphs: {len(testset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "trainloader = DataLoader(dataset = trainset, batch_size= batch_size, shuffle = True)\n",
    "testloader = DataLoader(dataset = testset, batch_size= batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Graph Neural Network (GNN)\n",
    "\n",
    "Training a GNN for graph classification usually follows a simple recipe:\n",
    "\n",
    "1. Embed each node by performing multiple rounds of message passing\n",
    "2. Aggregate node embeddings into a unified graph embedding (**readout layer**)\n",
    "3. Train a final classifier on the graph embedding\n",
    "\n",
    "There exists multiple **readout layers** in literature, but the most common one is to simply take the average of node embeddings:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{\\mathcal{G}} = \\frac{1}{|\\mathcal{V}|} \\sum_{v \\in \\mathcal{V}} \\mathcal{x}^{(L)}_v\n",
    "$$\n",
    "\n",
    "PyTorch Geometric provides this functionality via [`torch_geometric.nn.global_mean_pool`](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.glob.global_mean_pool), which takes in the node embeddings of all nodes in the mini-batch and the assignment vector `batch` to compute a graph embedding of size `[batch_size, hidden_channels]` for each graph in the batch.\n",
    "\n",
    "The final architecture for applying GNNs to the task of graph classification then looks as follows and allows for complete end-to-end training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(16, 64)\n",
      "  (conv2): GCNConv(64, 64)\n",
      "  (conv3): GCNConv(64, 64)\n",
      "  (lin): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_max_pool\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(16, hidden_channels, add_self_loops=False)                   # 16-->64 node features\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels, add_self_loops=False)      # 64-->64 node features\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels, add_self_loops=False)      # 64-->64 node features\n",
    "        self.lin = Linear(hidden_channels, 1)                                             # 64-->1 node features fully connected layer\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        # Take the sum over all nodes in each graph = 16 values per graph\n",
    "        x = global_max_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=64)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we again make use of the [`GCNConv`](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv) with $\\mathrm{ReLU}(x) = \\max(x, 0)$ activation for obtaining localized node embeddings, before we apply our final classifier on top of a graph readout layer.\n",
    "\n",
    "Let's train our network for a few epochs to see how well it performs on the training as well as test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\pyproj\\mt\\mt_env\\lib\\site-packages\\torch\\nn\\modules\\loss.py:530: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Double but expected Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [27], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m out \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39mx, data\u001b[39m.\u001b[39medge_index, data\u001b[39m.\u001b[39mbatch)  \u001b[39m# Perform a single forward pass.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m loss \u001b[39m=\u001b[39m criterion(out, data\u001b[39m.\u001b[39my)  \u001b[39m# Compute the loss.\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()  \u001b[39m# Derive gradients.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m optimizer\u001b[39m.\u001b[39mstep()  \u001b[39m# Update parameters based on gradients.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()  \u001b[39m# Clear gradients.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\david\\pyproj\\mt\\mt_env\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\david\\pyproj\\mt\\mt_env\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Found dtype Double but expected Float"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)#, momentum=0.9)\n",
    "\n",
    "iters, losses = [], []\n",
    "\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0\n",
    "    for data in trainloader:  # Iterate in batches over the training dataset.\n",
    "        out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        running_loss += loss\n",
    "\n",
    "    epoch_loss = running_loss/batch_size\n",
    "    losses.append(epoch_loss) # compute *average* loss\n",
    "    iters.append(epoch+1)\n",
    "    print(f'epoch {epoch + 1}, loss {epoch_loss:f}')\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# plotting\n",
    "#plt.title(\"Training Curve (batch_size={}, lr={})\".format(batch_size, learning_rate)) \n",
    "#plt.plot(iters, losses, label=\"Train\")\n",
    "#plt.xlabel(\"Iterations\")\n",
    "#plt.ylabel(\"Loss\")\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mt_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fed15e5751565248da5e6e03ce20f4f975d995c4be1c0cba0751577edee5982f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
