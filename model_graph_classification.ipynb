{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\pyproj\\mt\\mt_env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Install required packages.\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\david\\\\pyproj\\\\mt\\\\models'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'c:/Users/david/MT_code/data/extracted_patches/mutant_graphs_classification/'\n",
    "path = os.path.join(data_dir, 'AAAH_GraphPatch.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\david\\\\pyproj\\\\mt\\\\models'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'f_helper_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mf_helper_functions\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      3\u001b[0m patch \u001b[39m=\u001b[39m load_object(path)\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(patch\u001b[39m.\u001b[39mdistance_matrix\u001b[39m.\u001b[39mshape)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'f_helper_functions'"
     ]
    }
   ],
   "source": [
    "from f_helper_functions import *\n",
    "\n",
    "patch = load_object(path)\n",
    "print(patch.distance_matrix.shape)\n",
    "print(patch.edge_index.shape)\n",
    "print(patch.edge_weight.shape)\n",
    "print(patch.fitness)\n",
    "print(patch.A.shape)\n",
    "print(patch.mutant)\n",
    "print()\n",
    "print(patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import add_self_loops\n",
    "\n",
    "\n",
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.mutants = [mutant[0:4] for mutant in os.listdir(data_dir)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.data_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = os.path.join(self.data_dir, self.mutants[idx]+'_GraphPatch.pkl')\n",
    "        patch = load_object(path)\n",
    "        \n",
    "        x = torch.from_numpy(patch.features)\n",
    "        y=torch.from_numpy(patch.fitness.astype(np.int64))\n",
    "        pos=torch.from_numpy(patch.coords)\n",
    "\n",
    "        edge_weight=torch.from_numpy(patch.edge_weight)\n",
    "\n",
    "        edge_index=torch.from_numpy(patch.edge_index)\n",
    "        edge_index, edge_weight = add_self_loops(edge_index, edge_weight, fill_value=0)\n",
    "\n",
    "        return  Data(x, edge_index, edge_weight, y, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = PatchDataset(data_dir = data_dir)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: PatchDataset(1500):\n",
      "====================\n",
      "Number of graphs: 1500\n",
      "\n",
      "Data(x=[1049, 16], edge_index=[2, 6435], edge_attr=[6435, 1], y=1, pos=[1049, 3])\n",
      "=============================================================\n",
      "Number of nodes: 1049\n",
      "Number of node features: 16\n",
      "Number of edges: 6435\n",
      "Average node degree: 6.13\n",
      "Contains self-loops: True\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of node features: {data.num_node_features}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Contains self-loops: {data.has_self_loops()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3675],\n",
       "        [0.6447],\n",
       "        [0.6881],\n",
       "        ...,\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 1200\n",
      "Number of test graphs: 300\n"
     ]
    }
   ],
   "source": [
    "n_train = len(dataset) * 0.8\n",
    "n_test = len(dataset) * 0.2\n",
    "\n",
    "trainset, testset = torch.utils.data.random_split(dataset, [int(n_train),int(n_test)])\n",
    "\n",
    "print(f'Number of training graphs: {len(trainset)}')\n",
    "print(f'Number of test graphs: {len(testset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zeros in trainset: 349\n",
      "Number of ones in trainset: 851\n",
      "Fraction of ones in trainset: 0.709167\n"
     ]
    }
   ],
   "source": [
    "train_fraction = {0:0, 1:0}\n",
    "for step, training_sample in enumerate(trainset):\n",
    "    if training_sample.y == 0:\n",
    "        train_fraction[0] +=1\n",
    "    else: \n",
    "        train_fraction[1] +=1\n",
    "\n",
    "print('Number of zeros in trainset: {z}'.format(z = train_fraction[0]))\n",
    "print('Number of ones in trainset: {o}'.format(o = train_fraction[1]))\n",
    "print('Fraction of ones in trainset: {f:2f}'.format(f=train_fraction[1]/n_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zeros in testset: 88\n",
      "Number of ones in testset: 212\n",
      "Fraction of ones in testset: 0.706667\n"
     ]
    }
   ],
   "source": [
    "test_fraction = {0:0, 1:0}\n",
    "for step, test_sample in enumerate(testset):\n",
    "    if test_sample.y == 0:\n",
    "        test_fraction[0] +=1\n",
    "    else: \n",
    "        test_fraction[1] +=1\n",
    "\n",
    "print('Number of zeros in testset: {z}'.format(z = test_fraction[0]))\n",
    "print('Number of ones in testset: {o}'.format(o = test_fraction[1]))\n",
    "print('Fraction of ones in testset: {f:2f}'.format(f=test_fraction[1]/n_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "trainloader = DataLoader(dataset = trainset, batch_size= batch_size, shuffle = True)\n",
    "testloader = DataLoader(dataset = testset, batch_size= batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for data in enumerate(trainloader):\n",
    "#    print(data[1].x.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Graph Neural Network (GNN)\n",
    "\n",
    "Training a GNN for graph classification usually follows a simple recipe:\n",
    "\n",
    "1. Embed each node by performing multiple rounds of message passing\n",
    "2. Aggregate node embeddings into a unified graph embedding (**readout layer**)\n",
    "3. Train a final classifier on the graph embedding\n",
    "\n",
    "There exists multiple **readout layers** in literature, but the most common one is to simply take the average of node embeddings:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{\\mathcal{G}} = \\frac{1}{|\\mathcal{V}|} \\sum_{v \\in \\mathcal{V}} \\mathcal{x}^{(L)}_v\n",
    "$$\n",
    "\n",
    "PyTorch Geometric provides this functionality via [`torch_geometric.nn.global_mean_pool`](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.glob.global_mean_pool), which takes in the node embeddings of all nodes in the mini-batch and the assignment vector `batch` to compute a graph embedding of size `[batch_size, hidden_channels]` for each graph in the batch.\n",
    "\n",
    "The final architecture for applying GNNs to the task of graph classification then looks as follows and allows for complete end-to-end training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(16, 64)\n",
      "  (conv2): GCNConv(64, 64)\n",
      "  (conv3): GCNConv(64, 64)\n",
      "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(16, hidden_channels)                   # 16-->64 node features\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)      # 64-->64 node features\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)      # 64-->64 node features\n",
    "        self.lin = Linear(hidden_channels, 2)                       # 64-->2 node features fully connected layer\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        # Take the mean over all nodes in each graph = 16 values per graph\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=64)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we again make use of the [`GCNConv`](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv) with $\\mathrm{ReLU}(x) = \\max(x, 0)$ activation for obtaining localized node embeddings, before we apply our final classifier on top of a graph readout layer.\n",
    "\n",
    "Let's train our network for a few epochs to see how well it performs on the training as well as test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Acc: 0.7092, Test Acc: 0.7067\n",
      "Epoch: 002, Train Acc: 0.7092, Test Acc: 0.7067\n",
      "Epoch: 003, Train Acc: 0.7092, Test Acc: 0.7067\n",
      "Epoch: 004, Train Acc: 0.7092, Test Acc: 0.7067\n",
      "Epoch: 005, Train Acc: 0.7092, Test Acc: 0.7067\n",
      "Epoch: 006, Train Acc: 0.7092, Test Acc: 0.7067\n",
      "Epoch: 007, Train Acc: 0.7158, Test Acc: 0.7300\n",
      "Epoch: 008, Train Acc: 0.7192, Test Acc: 0.7333\n",
      "Epoch: 009, Train Acc: 0.7092, Test Acc: 0.7067\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Javascript\n",
    "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
    "\n",
    "model = GCN(hidden_channels=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    for data in trainloader:  # Iterate in batches over the training dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "         loss = criterion(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  \n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    loss = train()\n",
    "    \n",
    "    train_acc = test(trainloader)\n",
    "    test_acc = test(testloader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mt_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fed15e5751565248da5e6e03ce20f4f975d995c4be1c0cba0751577edee5982f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
